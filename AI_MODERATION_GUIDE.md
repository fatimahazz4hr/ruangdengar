# AI Content Moderation - Ruang Dengar

## ğŸ“‹ Overview

Sistem **AI Content Moderation** telah diintegrasikan ke dalam Ruang Dengar untuk secara otomatis menganalisis laporan yang masuk. Sistem ini menggunakan **Detoxify** (berbasis BERT) untuk analisis toxicity dan keyword-based classification untuk kategori insiden.

---

## ğŸ¤– Teknologi

### Libraries
- **Detoxify 0.5.2**: Open-source content moderation library berbasis Transformers
- **PyTorch 2.9.1**: Deep learning framework backend
- **Transformers (Hugging Face)**: Pre-trained BERT model

### Model Details
- **Model**: `unitary/toxic-bert` (original)
- **Size**: ~418 MB (diunduh otomatis saat pertama kali digunakan)
- **Accuracy**: ~85% pada dataset Jigsaw Toxic Comment Classification
- **Privacy**: Offline/on-premise, tidak mengirim data ke server eksternal

---

## âœ¨ Fitur Utama

### 1. **Automatic Toxicity Detection**
Menganalisis teks laporan untuk mendeteksi:
- Toxicity (komentar yang toxic/berbahaya)
- Severe toxicity (toxicity ekstrem)
- Obscene (konten cabul)
- Threat (ancaman)
- Insult (penghinaan)
- Identity attack (serangan identitas)

### 2. **Urgency Classification**
Laporan diklasifikasikan menjadi 4 tingkat urgensi:
- ğŸš¨ **DARURAT**: Situasi yang memerlukan respons segera
  - Indikator: bunuh diri, hamil akibat perkosaan, disekap, sedang berlangsung
  - Toxicity score > 0.7 atau severe_toxicity > 0.7 atau threat > 0.6
- âš ï¸ **TINGGI**: Memerlukan perhatian prioritas
  - Toxicity score 0.4 - 0.7
- ğŸ“Œ **SEDANG**: Perlu ditindaklanjuti
  - Toxicity score < 0.4
- âœ… **RENDAH**: Tidak mengandung konten berbahaya

### 3. **Keyword-based Category Classification**
Laporan dikategorikan berdasarkan keyword matching:
- **Kekerasan Seksual**: perkosa, cabul, dipaksa berhubungan, etc.
- **Pelecehan Fisik**: pukul, tampar, tendang, cekik, etc.
- **Pelecehan Verbal**: ancam, intimidasi, maksa, menghina, etc.
- **Pelecehan Psikologis**: manipulasi, gaslighting, kontrol, etc.
- **Cyberbullying**: chat, DM, sosmed, revenge porn, etc.
- **Stalking**: mengikuti, menguntit, memata-matai, etc.
- **Lainnya**: Jika tidak cocok dengan kategori di atas

### 4. **Content Blur Recommendation**
Sistem merekomendasikan blur/peringatan untuk konten dengan:
- Severe toxicity > 0.8
- Sexually explicit > 0.7
- Threat > 0.8

---

## ğŸ—„ï¸ Database Schema

5 field baru ditambahkan ke model `Laporan`:

```python
class Laporan(models.Model):
    # ... existing fields ...
    
    # AI Content Moderation fields (added in migration 0015)
    ai_kategori = models.CharField(
        max_length=50, 
        null=True, 
        blank=True,
        help_text="Kategori otomatis dari AI"
    )
    ai_urgency = models.CharField(
        max_length=20,
        choices=[
            ('darurat', 'Darurat'),
            ('tinggi', 'Tinggi'),
            ('sedang', 'Sedang'),
            ('rendah', 'Rendah'),
        ],
        null=True,
        blank=True,
        help_text="Tingkat urgensi dari AI"
    )
    ai_toxicity_score = models.FloatField(
        null=True,
        blank=True,
        help_text="Skor toxicity 0-1 dari Detoxify"
    )
    ai_needs_blur = models.BooleanField(
        default=False,
        help_text="Apakah konten perlu di-blur"
    )
    ai_analyzed = models.BooleanField(
        default=False,
        help_text="Apakah sudah dianalisis AI"
    )
```

---

## ğŸ”„ Workflow

### 1. Submission (users/views.py - `buat_laporan_view`)
```python
# Setelah laporan dibuat
laporan = Laporan.objects.create(...)

# AI Moderation dipanggil
from .ai_moderation import moderate_laporan
ai_result = moderate_laporan(deskripsi, jenis)

# Hasil disimpan ke database
laporan.ai_kategori = ai_result.get('kategori')
laporan.ai_urgency = ai_result.get('urgency')
laporan.ai_toxicity_score = ai_result.get('toxicity_score')
laporan.ai_needs_blur = ai_result.get('needs_blur')
laporan.ai_analyzed = True
laporan.save()
```

### 2. Admin View (templates/dashboard/kelola_laporan.html)
Admin melihat:
- **Urgency Badge** dengan warna berbeda per level
- **AI Kategori** di bawah urgency badge
- **Sorting otomatis**: Laporan darurat muncul di atas

### 3. Detail View (templates/dashboard/laporan_detail_admin.html)
Admin melihat:
- **AI Analysis Card** dengan border ungu
- **Urgency badge** dengan emoji
- **Toxicity progress bar** dengan warna dinamis (merah/kuning/hijau)
- **Content blur warning** jika `ai_needs_blur = True`

---

## ğŸ“Š Example Output

### Test Case
```python
from users.ai_moderation import moderate_laporan

result = moderate_laporan("Saya dipaksa melakukan hal yang tidak saya inginkan, dia selalu mengancam saya")
```

### Result
```python
{
    'kategori': 'Pelecehan Verbal',
    'urgency': 'rendah',
    'toxicity_score': 0.008,  # Low toxicity
    'needs_blur': False,
    'scores': {
        'toxicity': 0.008,
        'severe_toxicity': 0.0001,
        'obscene': 0.0006,
        'threat': 0.0002,
        'insult': 0.0005,
        'identity_attack': 0.0003
    },
    'analyzed': True
}
```

---

## ğŸ¨ UI/UX Features

### Kelola Laporan Table
- **Kolom AI Analisis** baru di antara "Jenis Insiden" dan "Tempat Kejadian"
- **Urgency badges** dengan animasi pulse untuk darurat
- **Color coding**:
  - Darurat: Red (#fee2e2)
  - Tinggi: Orange (#fed7aa)
  - Sedang: Yellow (#fef3c7)
  - Rendah: Blue (#dbeafe)

### Detail Laporan
- **AI Analysis Card** dengan background abu-abu dan border ungu
- **Toxicity bar** visual dengan percentage indicator
- **Content warning** jika perlu blur

---

## ğŸš€ Installation & Setup

### 1. Install Dependencies
```bash
pip install detoxify torch
```

### 2. Apply Migration
```bash
python manage.py makemigrations
python manage.py migrate
```

### 3. First Run
Model Detoxify akan diunduh otomatis (~418 MB) saat pertama kali digunakan.

### 4. Test
```python
python -c "from users.ai_moderation import moderate_laporan; print(moderate_laporan('test content'))"
```

---

## âš™ï¸ Configuration

### Menambah Keyword untuk Kategori
Edit `users/ai_moderation.py`:

```python
KATEGORI_KEYWORDS = {
    'Kekerasan Seksual': [
        'perkosa', 'cabul', # tambahkan keyword baru di sini
    ],
    # ... kategori lainnya
}
```

### Menyesuaikan Threshold Urgency
Edit fungsi `determine_urgency()`:

```python
if toxicity_scores.get('severe_toxicity', 0) > 0.7:  # Ubah threshold
    return 'darurat'
```

---

## ğŸ“ˆ Performance

- **Processing Time**: ~0.5-2 detik per laporan (tergantung hardware)
- **Memory Usage**: ~500 MB (model loaded)
- **Accuracy**: ~85% (dari Detoxify benchmark)
- **Offline**: Tidak memerlukan koneksi internet setelah model diunduh

---

## ğŸ”’ Privacy & Security

âœ… **Fully Offline**: Tidak ada data yang dikirim ke server eksternal  
âœ… **On-Premise**: Model berjalan di server lokal  
âœ… **No API Costs**: Gratis tanpa batas usage  
âœ… **GDPR Compliant**: Data tetap di infrastruktur sendiri  

---

## ğŸ› Troubleshooting

### Model Not Loading
```python
# Check if model file exists
import os
model_path = os.path.expanduser('~/.cache/torch/hub/checkpoints/toxic_original-c1212f89.ckpt')
print(f"Model exists: {os.path.exists(model_path)}")
```

### High Memory Usage
Model memerlukan ~500 MB RAM. Untuk server dengan RAM terbatas, pertimbangkan:
- Lazy loading (model hanya di-load saat digunakan) âœ… Sudah diimplementasikan
- Clear cache setelah batch processing

### Slow Processing
- Gunakan GPU jika tersedia (akan otomatis terdeteksi oleh PyTorch)
- Batch processing untuk multiple laporan:
```python
from users.ai_moderation import batch_moderate
results = batch_moderate(Laporan.objects.filter(ai_analyzed=False))
```

---

## ğŸ”® Future Enhancements

Potensial fitur yang bisa ditambahkan:
- [ ] Email notification ke admin untuk laporan darurat
- [ ] Dashboard analytics untuk kategori dan urgency trends
- [ ] Filter sidebar untuk urgency level
- [ ] Auto-assign counselor berdasarkan kategori
- [ ] Batch reanalysis untuk existing laporan
- [ ] Multi-language support (Detoxify mendukung bahasa lain)
- [ ] Custom trained model untuk bahasa Indonesia yang lebih akurat

---

## ğŸ“š References

- [Detoxify GitHub](https://github.com/unitaryai/detoxify)
- [Jigsaw Toxic Comment Dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)

---

## ğŸ‘¥ Team

Dikembangkan untuk sistem Ruang Dengar - Platform Pelaporan dan Konseling Kekerasan Seksual.

**Implementation Date**: Desember 2025  
**Version**: 1.0.0  
**Status**: âœ… Production Ready
